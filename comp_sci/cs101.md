# Computer Science 101

## Introduction

There are two kinds of knowledge.

1. Declarative knowledge is composed of statements of fact
2. Imperative knowledge states how to solve a problem

An algorithm is a description of how to perform a computation.

Programming languages are ways for computers to understand algorithms. They consist of instruction sets and a flow-of-control.

There are compiled and interpreted languages.

## Problem Solving

Broadly speaking, there are several types of ways to solve a problem.

### Exhausting Enumeration

Exhausting enumeration exhausts a list of possible answers within a problem set. This is colloquially referred to as *brute forcing* a solution, and this is often a good way to solve many problems since computers are fast.

### Approximation

For hard problems, like NP problems, it is more efficient to find an approximate solution that is "good enough". What does it mean for an answer to be good enough?

Lets say you are trying to solve for the square root of 2. The answer will be between 1.4 and 1.45. However, you could also be more precise and say that the answer is between 1.41 and 1.42. With approximation, you create what is called a *terminating value* where if an answer is within a predetermined range than the condition has been satisfied. This terminating condition is sometimes called the *specification* of the problem.

### Designing algorithms

When designing an algorithm you can improve efficiency by reducing the amount of input. For example, if your algorithm that is solving the square root of a number, and it considers every numerical increase (0, 0.00001, 0.00002, 0.00003, etc.) than the larger the number being tested the greater the volume of items that the algorithm needs to evaluate. There are two things you can do to improve the efficiency of such an algorithm.

1. Reduce the specification. If, for example, you instead consider (0, 0.0001, 0.0002, 0.0003, etc.) you have considerably reduced the number of items for the algorithm to consider.

2. At this point it is better to not solve the problem by brute force. A binary search method would arrive at a solution considerably faster.

## Functions and Machine Interpretations of Programming

A good programmer is measured by the functionality of their programs.

Functions solve two different issues in programming:

1. *Decomposition*: Functions create structure by breaking up the program into modules. They are self-contained and reusable.

2. *Abstraction*: Functions suppress details. They treat blocks of code as a black box where you no longer need to think about how it works, you only need to know how to use it.

Two things happen when a function is called:

1. The formal parameter is bound to the value of the actual parameter.

2. Upon entry of a function a new scope is created. A scope is a mapping from names to object.

## Objects

*Binding* is the assignment of names to objects. *Mutation* changes the value of those objects. Certain data structure collections will be immutable (Python's tuples, for example) and others are mutable (a JavaScript array).

An *alias* is a single object with multiple names. For immutable objects this presents no problems, but for mutable objects this is a potential way to create annoying bugs. As an example, imagine a function that takes two arrays or lists as parameters where it appends the contents of one list and to the second. Now imagine calling that function and using the same list for both parameters. That's an example of an alias, and in this example it creates an infinite loop since while the loop that appends elements is running it will discover an ever increasing length to the list.

## Recursion

Modular abstraction is desirable because small problems are easier to solve and solutions to small problems can easily be used to solve the original problem.

Recursion is a way of describing problems and designing solutions.

First there is a base case with a direct answer. Then there is the recursive case that reduces a complex problem into a simpler one.

## Debugging

The binary number system is significant since that is the number system a classical computer (i.e. non-quantum) utilizes. Computers work in binary because of switch positions in electronics. Designing an on/off switch makes the translation simple.

The difference between the decimal and binary system causes problems found in floating numbers. For example:

Expressing 0.125 in the decimal system: (1 * 10<sup>-1</sup>) + (2 * 10<sup>-2</sup>) + (5 * 10<sup>-3</sup>)

Expressing 0.001 in binary: 1 * 10<sup>-3</sup>

Expressing 0.1 in decimal: 1 * 10<sup>-1</sup>

Expressing 0.1 in binary involves in an infinite repeating binary function.

The takeaway from this is to never evaluate if two floating numbers are equal to each other. Use a specification variable to check if they are "close enough".

When debugging a program, it is good to utilize the scientific method.

- Ask how the results could have happened?
- Study the available data
- Form a hypothesis consistent with the available data
- Design and run a repeatable experiment (the nice thing about programming is most of the time your experiment should be repeatable)
- Make sure your experiment has the potential to refute your hypothesis

Some additional considerations for writing tests:

- Try to find smaller pieces of input that fail
- It is helpful to have statements evaluating what you think should happen (i.e. `"Should print True", bool`)

## Efficiency and Order of Growth

Efficiency is about choosing the right algorithm, not about specific coding details.

Algorithms are difficult to invent. Typically when faced with a novel problem, you try to reduce the scope of the problem until it resembles an already solved problem (i.e. one where an algorithm already exists).

Efficiency problems take on two dimensions: space & time, but practically speaking usually one is concerned about time.

There is no way to make stable measurements of computational complexity. It will depend on the speed of the machine, and other highly variable factors. Instead the number of steps taken to complete an instruction set are considered.

When considering the length of time a program takes there are three cases one could look at: best case, expected case, and worst case.

Best case is not helpful. When looking for an item in a list if it happens to be the first item that doesn't tell you a lot about the efficiency of the algorithm.

The expected case is also usually ignored since there isn't any straight forward way to model it.

Instead the focus is on the worst-case scenario, which provides the upper bounds of the time it takes an algorithm to solve a problem. This is in fact a very useful scenario to consider since it comes up a lot (for example, an algorithm tries to find an item in a list that isn't actually contained in the list).

Big O notation is so widely used because it takes into account these sorts of considerations.

Big O notation as it is utilized in computer science relies on a Random Access Machine (RAM) model of how the computer performs operations. The assumption of this model is:

1. Instructions are performed sequentially
2. Any random piece of memory can be accessed in constant time.

Before the invention of the hard drive when memory for computers was stored on reel-to-reel tape this would have been a bad model to use for evaluating the efficiency of an algorithm because the location of a piece of information on the tape would be of great practical importance. And the RAM model isn't quite how computers work today, in fact computers do have different hierarchies of cached memory that take different times to access. Nonetheless, the computers we work with today bear a close resemble to the RAM model, and so for practical purposes Big O notation is the right way to evaluate the efficiency of the algorithms we write.

## Memory and Search Methods

It is difficult to evaluate the size (in memory) of an object that contains multiple data types. Some data types have a fixed size: integers are whatever bit of memory they take up (32-bit and 64-bit are the most common), booleans are also a fixed size. However we don't know the size in advance of other data types. Strings, even if they are immutable, will vary on size depending on the number of characters. And then some data types are mutable, and that means it is impossible to know their size in advance.

A linked list has every element of a list as a pointer to the next element of the list, and the value. This makes the linked list efficient to write to, but inefficient to read from. So in practice the linked list is a rarely used data structure.

The implementation arrays in most languages instead relies on what is called *indirection*, or *dereferencing*. Indirection is when a piece of data is referenced using a name instead of the value, and this is a technique that is used a lot in object oriented programming.

Binary searches are O(log n) so they are very efficient, but only if they have a sorted list to begin with. The question is is it efficient to write code where the order of operations is:

1. Sort a list
2. Run a binary search

We already know the efficiency of the binary search, so the only question is what is the rate of growth for sorting a list? To efficiently sort a list you would need to be more efficient than linear growth i.e. sublinear search. In other words, it is impossible.

### Amortized complexity

However we can say that even if the sort operation is inefficient, if one references a sorted list enough times it will nonetheless be more efficient than if we were constantly looking up the unsorted list.

If we plan on k searches of list L we are evaluating the following:

O(sort(L) + k * log(len(L) < k(len(L))))

The next question is how well can we do sorting? (the answer should always be better than bubble sort).

### Selection Sort

Selection sort depends on establishing and maintaining invariants. The list is divided into a prefix and suffix. The invariant will always have the prefix sorted.

Each step will find the lowest value, and put it as the first element. The complexity of the method is that it looks at every element of the suffix. So it looks at n elements, then n - 1 elements, then n - 2 elements, etc. It amounts to roughly O(n<sup>2</sup>).

### Merge Sort

There is a better method for sorting that relies on divide & conquer principles.

1. Choose a threshold size
2. Choose how many instances are at each division.
3. We need an algorithm to combine the sub-solutions.

Given two sorted lists you can merge them quickly.

```
l1 = [1, 5, 12, 18, 19, 20]
l2 = [2, 3, 4, 17]
```

1. Compare the first elements of each list
2. Compare the first element that "lost" to the second element of the other list.

The number of copies is O(len(list)), but the question of how many comparisons is trickier to solve, but the worst case is still O(len(list)).

How many times would you perform a merge sort? The answer is O(n log n).

## Hashing, Exceptions and Classes

### Hashing

Hashing is how objects are implemented. They are efficient, but it comes at the cost of memory/space.

`hash(i) -> 0 -> k`

The property of a hash function is many-to-one. Any number of integers can get assigned to a fixed number of buckets. When integers have the same value in the same bucket collision occurs. Linear rehashing is one of the many ways to solve this problem.

The complexity of the hash is dependent on the length of the buckets.

### Exceptions

TypeOf errors, and other kinds of errors are exceptions. Unhandled exceptions will cause the program to crash.

try-except blocks are exceptions a program is designed to accept. If the exception is raised it will jump to a different block of code to work on. This is important for web servers, and other environments where you might be expecting data that for whatever reason is not available for the program to evaluate/handle.

### Classes

A class is a collection of data and functions that operate on that data. Classes are critical for object-oriented programming.

The data and functions associated with a class' objects are called its attributes.

Sometimes when people discuss Object-Oriented Programming people will make reference to a "message passing metaphor".

A method is a function associated with an object.

## Object-Oriented Programming and Inheritance

The advantage of object oriented programming is it gives us access to abstract data types. This allows us to extend the programming language we work in by using user defined types. Each type will have an interface that explains what the methods do.

Object-Oriented languages use key words like `this` or `self` as a local type to a class that can be extended to other objects of that class.

*Data hiding* is the concept that a user shouldn't have to access instance variables contained within a class, as well as class variables.

Subclasses inherit the properties of the super class. Subclasses are useful because they produce different types of objects that can be used to distinguish objects.

For example, there could be a Person class, with a subclass of University Students. And then among University Students there could be subclasses of Graduate Students and Undergraduate Students. University Students could have ID numbers that all instances share. Undergrads could have a year attribute, but Graduates would not have this attribute.

## Intro to Simulation and Random Walks

### Generators

A generator is a function that remembers the point in the function body where it last returned, plus local variables. Generators provide a useful way to control access to a collection.

### Scientific Inquiry

How do we build computational models to solve a problem?

For much of scientific history the focus was on inventing analytic methods. That is, a way to predict outcomes given initial conditions and parameters. This lead to Newtonian physics, calculus, and probability theory. In the 20th century it reached its limits, and so simulation methods were developed. Sometimes there are non-mathematically tractable models like weather forecasting. Something with complex systems it is better to successively refine a series of simulations.

Simulations are also good at extracting useful intermediate results. The utility of simulations for scientific research were all made practical by computers. Before computers, simulations could be constructed mechanically, but this was expensive and difficult to produce, for example Leonardo da Vinci created a model of the solar system with a complicated clockwork contraption (you can see examples of devices like this in the Galileo Museum in Firenze).

A simulation builds a model that gives useful information about behavior of a system. It is an approximation to reality. Simulation models are descriptive, not prescriptive.

Brownian motion is an example of a *random walk*. A random walk assumes a randomly distributed range to try and understand behavior.

## Basic Probability and Plotting Data

How much should be inferred from simulation results? Not too much. The results of programs are *stochastic*, which is to say probabilistic.

The role of randomness in computation is challenging for human intuition. The Copenhagen doctrine put an end to the deterministic worldview.

*Casual non-determination*: Not every event is caused by previous events. Einstein hated this view and argued for predictive non-determinism, essentially the statistical theory is a result of not knowing enough information about the prior states of being.

*Stochastic Processes* are when the next state depends on the previous state(s), and there is a random element.

Independence in randomness is important, one outcome should have no influence on the next. So when asking what a probability is we're asking what fraction of possible results contain this random independence property.

Computing the odds of something happening, and the odds of something not happening are equally useful.

2<sup>10</sup> is the range of ten coin flips.

1/<sub>2<sup>10</sub> is the odds for a particular coin flip.

1 - 1/<sub>2<sup>10</sub> is the odds of a particular coin flip *not happening*.

Useful links for simulation analysis:

[SciPy](https://scipy.org/getting-started.html)

[MatPlotlib](matplotlib.sourceforge.net)

## Sampling and Monte Carlo Simulation

If a die is rolled 10 times what is the probability that a 1 will never be rolled? There are 6<sup>10</sup> possible permutations. So then the question is among those collections, how many instances don't contain a 1?

In a single roll the odds are 5/<sub>6</sub>. On the second roll the odds are 5/<sub>6</sub> * 5/<sub>6</sub>. On the third roll the odds are 5/<sub>6</sub> * 5/<sub>6</sub> * 5/<sub>6</sub>. etc.

Therefore the odds of getting at least one 1 is 1 - (5/<sub>6</sub>)<sup>10</sup>.

Solving a probability question by solving for not x or - 1 is a common technique.

Almost all of early probability theory was dedicated to solving gambling problems. Pascal is considered foundational to the field. A typical inquiry would be is it profitable to roll a pair of dice 24 times to roll a pair of sixes?

1/<sub>6</sub> * 1/<sub>6</sub> = 1/<sub>36</sub> for the first roll. So (35/<sub>36</sub>)<sup>24</sup> is roughly equal to 0.51.

Is it easier to write a simulation, or to calculate the probabilities? If possible, doing both is the best approach since it will help verify your work.

A *Monte Carlo simulation* uses repeated sampling to obtain the statistical properties of some phenomenon. The phrase was coined in 1949 to refer to a method from 1946.

*Inferential statistics* is based upon the guiding principle that a random sample tends to exhibit the same properties as the population from which it is drawn.

It is important to raise questions about sampling methods. Incorrect sampling will lead you astray.

The greater the number of simulations, the more stable the range of results. This is also known as *the law of large numbers*. It states that in repeated *independent* tests with the same actual probability (p) of an outcome for each test, the chance that the fraction of times that outcome occurs converges to p as the number of trials goes to infinite.

You can never be totally certain about sampling results. What techniques can be used to determine certainty within a range?

## Statistical Thinking

How many samples do we need to believe an answer?

Variance is the measure of how much spread there is in the possible outcomes. Thus multiple trials are critical. One trial that makes one hundred attempts is not as good as ten trials that make ten attempts.

Standard deviation measures the fraction of values that are close to the mean.

The coefficient of variation is used to measure whether the variation is relatively acceptable. It is the standard deviation divided by the mean.

If standard deviation is less than 1 it is low variance. If the mean is 0 small changes will lead to large variance, but that isn't really meaningful.

The coefficient of variation cannot be used for confidence intervals.

Normal distribution always peaks at the mean, and falls off symmetrically. The shape is a bell curve.

Normal distributions are used to construct models because:

1. they have nice mathematical properties
2. there are many naturally occuring instances.

They are characterized by two parameters: the mean, and the standard deviation. Knowing these we can calculate confidence intervals.

A confidence interval creates estimates by providing a range likely to contain the unknown value, and a confidence level that the value lies within that range.

*Example*: Candidate (D) likely to get 52% ± 4% of votes.

Note that this assumes that trials of elections have normal distributions.

*The empricial rule*: 68% of data is within 1 standard deviation of means.

The standard error is an estimate of the standard deviation. It assumes a normal distribution and that the sample population is small relative to the actual population.

If we assume:

p= % of population sampled
n= sample size

then...
standard error = ((p * (100 - p) / n))

Many experimental setups have normally distributed measurement errors.

## Using Randomness to Solve Non-Random Problems

How do we construct computational models that we can use to understand the real world?

A Gaussian distribution is one way to understand possible outcomes. Another name for this is a normal distribution, and it is also informally referred to as the bell curve.

A uniform distribution is where each result is equally probable. The range is all that you need to know about a uniform distribution. This type of distribution is common in gambling systems, but not in naturally occuring systems which tend to have a normal distribution.

Exponential distributions have the property of being memory-less. At each step the property is independent of previous steps.

Step 1: (1 - p)
Step 2: (1 - p)<sup>2</sup>
Time T: (1 - p)<sup>T</sup>

In evaluating models we evaluate fidelity (i.e. the credibility of the model, and utility i.e. what questions are answerable with the model?)

In contrast to analytic models, simulation models often have additional utility, but they often have comparable fidelity.

Many physical systems involve exponential decay and growth, for instance half-life decay or algae population growth.

Randomized algorithms have a purpose for non-random problems.

Pi is a known number that mathematicians have tried to solve for. Egyptians figured it to be 3.16, which was a fairly good answer. The Bible supposed it to be 3 (less good). Archimedes tried something slightly more sophisticated and created an approximation providing an upper and lower bound.

223/<sub>71</sub> < Pi < 22/<sub7</sub> ≈ 3.1418

In the 17th century two French mathematicians created a stochastic simulation. They created a circle within a square, and dropped needles in, counting the number of needles inside the circle, and the number outside the circle but within the square. From that they derived the formula:

needles in circle/<sub>needles in square</sub> = area of circle/<sub>area of square</sub>

From that formula they reasoned:

Pi = area of circle = area of square * needles in circle/<sub>needles in square</sub>

They ran into a problem though. They didn't have enough needles to arrive at a good estimate. However the model for the simulation was sound. The more simulations you can run, the smaller the standard deviation becomes, which increases your confidence in the answer.

## Curve Fitting

Modern science considers the interplay of physical reality, theoretical models, and computational models.

Hook's law calculates spring force, which turns out to have a lot of applications. The formula is:

f - kx

Where f is the force, and k is the spring constant. If you take a spring and attach a weight of a known quantity you can calculate for k.

Experimental errors are a practical reality. So using multiple weights to give you more data points is valuable.

To determine the best fit for data we need an objective function. There is a standard typically used called the *least squares fit*. It amounts to the formula:

(observed points[i] - predicted points[i])<sup>2</sup>

For every x value there should be a predicted y value.

polyFit is a built in function to pyLabs that finds a fit. It takes 3 arguments:

1. All of the observed x values
2. All of the observed y values
3. The degree of the polynomial

Possible outcomes could include:

- A line (y = ax + b)
- A parabola (y = ax<sup>2</sup> + bx<sup>2</sup> + c)

Linear regression can be used to find polynomials besides lines.

In addition to linear fit there is also cubic fits, etc.

It is possible to fit curves that have terrible predictive value. This is an issue that cannot be answered by statistics. One has to return to what the theoretical proposition states. There is a formula called the coefficient determination that is used to determine the quality of a fit. It is represented by R<sup>2</sup>, and the formula is:

R<sup>2</sup> = 1 - EE/<sub>MV</sub>

Where EE stands for estimated error, and MV stands for measured variance. This produces a value between 1 and 0, where 1 represents absolute correlation, and 0 represents no correlation at all.

Once you have a good model you can ask why do we care? The purpose is to use the model to ask questions about real instances. The pattern that is used in scientific and engineering experiments is as follows:

1. Start with an experiment that provides you with data about the behavior of a particular system.

2. Use computation to find and evaluate a model. Evaluation is important because we care about the quality and validity of the model.

3. Use theory, analysis, and additional computation to derive a consequence of the model.

## Optimization Problems and Algorithms

Optimization problems, as the name suggests, are problems where you are trying to perform some actions that need to be optimized. They consist of two parts:

1. An *objective function*, that is, what is the thing you are trying to optimize? You are going to look at either maximizing or minimizing some value. For example, what is the fastest way to arrive from point A to point B using public transportation? Alternatively you could be looking to minimize the amount of money you are spending.

2. A *set of constraints*. For example, you cannot spend more than a fixed amount of money travelling a particular distance.

Like other problems, the typical approach to take is one of *problem reduction*, that is, take a new problem and map it to an existing, solved problem.

With optimization problems you need to think about how hard they are. Optimization problems can feature NP complexity, and in those instances you may need to find approximate answers so that the problem can be solved in a reasonable amount of time.

A classic optimization problem is *the knapsack problem*. In the knapsack problem you can fix a certain amount of items of various values and weights into a knapsack that has a particular weight capacity. How do you maximize the value of the items that are going into the knapsack?

The first approach is to use a greedy algorithm. A greedy algorithm has a characteristic of being iterative, and simply searching for a locally optimal solution, that is, it only cares about the optimal solution at any particular step. This immediately begs the question of what counts as locally optimal?

There are in fact different types of knapsack problems. There is the 0/1 knapsack problem, as well as the continuous knapsack problem. The 0/1 problem involves taking whole items, so an item is either taken, or it isn't. The continuous knapsack problem doesn't have this constraint, and therefore is easier to solve (you take as much of the most valuable item as you can, then move onto the next most valuable, etc.) We will be looking at the 0/1 problem since that poses more interesting questions.

A greedy algorithm function could have 3 parameters: items, knapsack carry capacity, and a "key function". The key function is simply a definition of what counts as locally optimal. You could have three possible theories of what counts as locally optimal:

1. Select the item of the greatest value.

2. Select the item with the least weight so that you can maximize the items you pick up.

3. Calculate and select items that provide the best value/weight ratio.

The efficiency of a greedy algorithm is close to O(n), so it is very efficient. This is the nice thing about greedy algorithms, they are efficient and easy to code. The downside is they do not always result in what is globally optimal. One issue the knapsack problem exposes is that what counts as locally optimal will vary depending on the carry capacity of the knapsack. So in this instance it is worth thinking more about how we might formulate what is globally optimal.

One way is to brute force the problem. We can take the following steps:

1. Represent each item by a pair where I = (value, weight)

2. Have W as the carry capacity for the knapsack.

3. Have an available items vector called AI.

4. Have a selected items vector called SI.

So the new objective function will have the following formula:

`SI[i] * AI[i].value`

And it will be subject to the following constraint:

`AI[i] * SI[i].weight <= W`

What is the efficiency of this? Stated another way, how many possible vectors are there? It will be O(2<sup>n</sup>). If n is 5 this might not be so bad, but if it is 500 this is incredibly bad, and cannot be solved in a reasonable amount of time.

To solve this problem we will end up relying on the concept of a *powerset* which is the set of all subsets. The knapsack problem is inherently exponential. That is to say, the *worst case scenario* is inherently exponential. Clever algorithms can get around this by avoiding the worst case scenario.

## Clustering

Quick terminology note: *centroid* means the mean position of all points in a shape. This is an important concept for clustering problems.

*Machine learning* is difficult to define. One could define it as a machine indirectly acquiring information, that is to say, it is not directly being fed information from via the programmer's script. This relies on a process of inductive inference where the program observes incomplete information about particular phenomena, and tries to develop a model to predict outcomes.

There are two types of machine learning: supervised and unsupervised learning.

*Supervised learning* involves associating a label with each example in a training set. If each label is discrete this is called a classification problem. If the label is a real value this is a regression problem.

There are several questions you have to ask with regards to supervised learning.

- Are the labels accurate? In real scenarios, the answer is often no.
- Is the past representative of the future? You could employ supervised machine learning to study trends in the pre-2008 U.S. housing market, but it turns out that even though the data you analyzed was accurate, and correct conclusions were extracted from that data, it would not be representative of the future.
- Do you have enough data to generalize? A big aspect here is *feature extraction* i.e. creating separations based on abstractions derived from one observed phenomenon.
- What should the fit be?

*Unsupervised learning* is the training set data without the labels. What can you learn from that? You learn about the regularities of the data.

The dominant form of unsupervised learning is *clustering*. Clustering is the process of organizing data points into groups where they are similar in some way. Clustering algorithms are used a lot for large retailers to discover consumer behavior. Biologists can use it to find groups of genes.

Clustering is an optimization problem, so its qualities are:

- It should have low intra-cluster dissimilarity.
- It should have high inter-cluster similarity.

Variance can be used to measure this. Some constraints are needed to develop a useful cluster formulation, otherwise you end up with clusters that contain a single element, which doesn't tell you anything. A useful constraint can be a maximum number of clusters (like in the knapsack problem). Because cluster algorithms are by nature computationally intensive, they often rely on greedy algorithms. It relies on two in particular:

1. Hierarchical
2. K-Means

Hierarchical clustering has a set of n items to be clustered, and there is a n * n distance matrix.

1. Assign each item to its own cluster so there are n clusters for n items.
2. Find the most similar pair of clusters and merge them.
3. Continue until there is a single cluster, or the constraint condition is reached.

This is called agglomerative clustering because it combines items.

What does it mean to find the most similar pairs in step 2? We rely on *linkage criteria* to answer this question. There is *single linkage* where you compare the shortest distance from any cluster member to any other member. There is also *complete linkage* where you consider the furthest distance. There is also *average linkage* that examines the mean or median distance.

Some weaknesses of hierarchical clustering:

- Time consuming and doesn't scale well. Usually O(n<sup>2</sup>), sometimes worse.

- As is the issue with so many greedy algorithms, it doesn't always find the optimal solution.

When dealing with multidimensional data you need a feature vector. This would account for different data set variables. For example a feature vector for cities could include distance, population, GPS coordinates, etc. But what do these different types of data have to do with each other (that is to say, sometimes nothing)? This raises feature extraction questions.

K-Means clustering involves knowing how many clusters you want. The k refers to the number of groups you are clustering to. Therefore you designate the number of groups, and then among the data points you start finding the means based on the number of groups.

There's no guarantee that there will be convergence with k-means clustering, even though convergence usually occurs quickly. Centroids are often picked at random. This is not the best selection method, but it at least guarantees against systematic error in the selection process.

Machine learning recap:

- With supervised learning you start with a training set with labels. With this data you try to infer the relationship between the points and the associated labels.

- With unsupervised learning you have a training set that has unlabeled data. So you try to infer relationships among points. You infer these relationships via clustering algorithms.

- Both types of learning are similar to regression where you try to fit curves to data. You always need to be careful about overfitting. In particular, if the training data is small you may learn things that are true of the training data that are not true of a larger data set.

- Features matter!!! It matters if the features are normalized, and it matters if the features are weighted. They need to be relevant to the type of knowledge you are trying to acquire.

## Using Graphs to Model Problems

One of the most common types of models is the *graph theoretic*.

Suppose there is a list of airlines to all the U.S. cities with their price. And also suppose that cities with names that start with A, B, or C cost the same. What types of questions would you ask about this data set?

The types of questions you might ask would involve certain questions about costs and distances. These questions can be handled by graphs.

A graph is a set of nodes (sometimes referred to as vertices). They are connected by a set of edges (sometimes called arcs). If the edges are unidirectional, it is called a digraph (short for directed graph).

Graphs are used where there are interesting relationships between the nodes and edges. The first documented graph was in 1735 by a Swiss mathematician, Euler, to formulate a solution to the Konigsberg bridge problem.

Graph theory helped simplify this problem. Each landmass became a point, and each bridge was a line. From the graph he surmised that this was only solvable if the number of edges for each node was even.

Weights are edges that have values rather than just control flow between nodes. These could be costs, distances, etc.

Things you can model with graphs: a highway system, the web, etc. There are also less obvious applications like protein interactions, gene expressions, phase transitions, diseases, etc.

### Implementing a Graph

There are two common representations for a graph:

1. An adjacency matrix. If you have n nodes, then you have an N x N matrix. This might not be sufficient if there are multiple edges to the same node. They are good though if the connections are dense where every node connects to every other node.

2. An adjacency list. This lists every edge out.

The simplest graph for an adjacency list involves creating two classes: a node and an edge, and the graph itself (this could involve more than one class).

The Node class contains a name for the node.

The Edge class contains a source (i.e. the node it is from), destination (node it goes to), and weight.

Finally, the Digraph class contains a set of nodes, a dictionary of edges. It also has methods for adding nodes, edges, checks children of, and if it has a node. This could be a digraph. That could be the child of a parent class called Graph.

### Formulating Problems as Graph Problems

The most common problem to solve using graphs is the shortest path problem. A similar, slightly more complex problem is the shortest weighted path problem.

An even more complicated problem to solve is finding cliques. This involves finding a set of nodes such that there exists a path connecting each node in the set.

Then there is also the minimum cut problem (min cut). Given a graph, and given two sets of nodes find the minimum number of edges such that if those edges are removed the two sets are disconnected. i.e. you cannot get from one set to the other set. This is a problem that power line implementation has to solve by creating redundancies such that the min cut number isn't too small.

Questions to ask:

- Does there exist a node such that node has some property and is connected to every node with that property? If such a node does not exist that means there is not a single source for that property.

- Is there a connection from node x to node y? If yes, what is the shortest path? How many nodes do you have to go through? This is sometimes referred to as the six degrees of separation formulation.

### Depth First Search Algorithm

The shortest path problem is solved with a depth first search(DFS) algorithm. It looks from the start point, and looks at each child node. The recursion ends when `start == end`, that is the start node and the end node are identical (this prevents cycling issues where you are traversing the same nodes repeatedly). The recursive part begins by choosing one child of the current node, and it keeps doing that until it:
1. reaches a node with no children, or
2. it reaches the node you are looking for, or
3. it reaches a node you have already visited.

This method systematically explores all paths, and then selects the best one. Unfortunately this sort of algorithm suffers from a similar problem with optimization problems: it features exponential growth as the density of nodes and edges increases. This is because it is constantly reexamining paths it has already considered. To solve this problem requires *memoization* i.e. table lookup. This is the heart of *dynamic programming*.

## Dynamic Programming

Dynamic programming was invented by Richard Bellman in 1950. The name was deliberately designed as a piece of misdirection. What it actually is is a mathematical solution to solve problems that appear exponential, but create a quick solution.

When can dynamic programming be used? It can be used for problems with two properties:

1. The problem has an optimal substructure. That means you can find a globally optimal solution by combining locally optimal solutions.

2. The problem has overlapping subproblems. This means that you solve the same subproblem more than once. Therefore you can use memoization for table lookup to help solve the problem.

Sometimes a problem will feature one of these properties but not both. For example, a sort problem will feature an optimal substructure, but it does not feature overlapping subproblems, that is, once you have a sorted element, you do not return to it.

To analyze if a problem has these two properties there are questions you can ask. For example, with the shortest path problem we can ask if I know the shortest path from node A to node B, and the shortest path from node B to node C, do I now know the optimal solution to find the shortest path from A to C? The answer is no, because there could be a path that goes directly from A to C that is shorter than the path from A to B to C.

However if you already know the shortest path from two nodes, and then analyze subpaths from these two nodes, then you do know shortest paths from these nodes, so in that instance there is an optimal substructure. So under those conditions there are instances where the shortest path problem is amenable to dynamic programming solutions.

The 0-1 knapsack problem also can be solved using dynamic programming. You can create a decision graph for the selected items for the knapsack, and in that way we have a problem with overlapping subproblems. That is, given a set of items available at a given node, given an available weight which items should be selected? Importantly with the knapsack problem, we have to consider the weight of items we have already selected, although we are not concerned with how those items were selected.

What is the rate of growth using dynamic programming to solve the 0-1 knapsack problem? It depends on the number of items (n), the number of available items (a subset of n), and the number of specified weights(w).

So even with dynamic programming we still do not get away from the exponential growth that is inherent to the problem. Dynamic programming is sometimes said to have pseudo-polynomial time. That is, sometimes it solves the problem with a polynomial time rate of growth, but not always.

## Avoiding Statistical Fallacies

Statistics as a field of inquiry first showed up in the middle of the 15th century in England as a way to model the spread of the plague.

Statistics are a powerful tool, but there is always the possibility of mislead with statistics, either deliberately or out of ignorance, incompetence, and/or general human failure.

1. Statistical measures don't tell the entire story. Don't ever ignore the data. You can have dramatically different data points that have linear regressions that look identical.

2. Pictures can be deceiving. Think about housing prices in the Midwest from 2006 - 2007 you get a picture of stability. But then imagine adding 2008 to your data set! Suddenly your picture looks dramatically different. Also you plot your x and y axis is another trick that can be employed. Imagine x is linear growth, and y is logarithmic growth. This is an easy way to be misleading. Similarly, the range of your axis can create dramatically different impressions.

3. The most serious and common statistical error consists of the garbage in and garbage out problem(GIGO). If your data is bad, your results are bad.

4. *Cum hoc ergo propter hoc* fallacy. "With this, therefore because of this". Put another way, one unwarrantedly assumes a causality based on a correlation. In fact, causation is much more difficult to prove. One always has to consider if there is a lurking variable missing in your analysis.

5. Non-response bias, and the related problem of a non-representative sample are another issue that plague statistics. Statistics is predicated on the idea that a random sample of a population that are applicable to the population at large. However this assumption falls apart there is a significant non-response bias, or non-representative sample. Convenience selections prove an issue here. This is a well known issue in political polling. For example, if you only call landlines you are less likely to contact younger voters, who have very different voting patterns from older voters. This is because younger voters are significantly less likely to own a landline. Failure to account for this discrepancy results in faulty analysis.

6. Data enhancement. It is easy to read into data things that the data does not necessarily imply. For example, there was a social panic over swine flu because of a report the there were 246 deaths attributable to it. But annual deaths attributable to the common flu was in the thousands, but this important piece of context was rarely provided in media reports.

7. "The Texas Sharpshooter Fallacy". This involves moving the goalpost so that your data looks more impressive than it actually is. The name of the fallacy refers to a joke about a Texan who fires gunshots at the side of a barn, and then paints a bullseye around them to claim to be a sharpshooter. This is why it is important to first formulate a hypothesis, and then run an experiment, rather than the other way around.

## Queuing Network Models

*Queuing networks* give us a formal way to study systems where waiting is a fundamental feature of the system.

Why do we have queues? For reasons of economy. When resources are limited, it makes sense to allocate workers to a workload such that they do not idle. Typically a balance is struck between customer service and resource utilization. In a complex system there is a need for excess service capacity since demand can unpredictably spike.

When thinking of how to model a system with queues you start with jobs. Those jobs enter a queue. They wait, leave one at a time, enter a server, and then depart.

There are several things to analyze with this model:

- The arrival process. How do jobs appear? Do they arrive individually or in groups? A group is said to be a batch process.

- How are arrivals distributed in time? Is the arrival in uniform in temporal distribution, or is it random? This is modeled using what is called an inter-arrival time distribution. The simplest process is for a constant interval (uncommon). More likely is the Poisson process where there is exponential distribution. From this we can describe the average arrival rate.

- The service mechanism. How does the server work? How long will it take to provide service? This is called the service time distribution. This depends on two things: 1. the amount of work the job entails, and 2. the speed of the server. The number of servers is another consideration. The number of queues is also relevant. Are there multiple queues (like at a supermarket) or a single queue (like at an airport). Single queues are actually more efficient generally. But there isn't always enough space for a single queue (like with the supermarket example). Another question is if we allow preemption. That is, can a server interrupt a process to handle some higher priority job?

- Queue characteristics. The fundamental question is the policy. How do we choose what job gets served next? There is First In, First Out (FIFO). There is also LIFO (Last In, First Out). In computing applications Shortest Remaining Processing Time (SRPT) is common. The choice of the queuing discipline can have an amazing affect on performance. SRPT is popular because it reduces queue congestion, improving expected service time. The downside of SRPT is its perceived unfairness. It allows "starvation", where the lengthy jobs never get serviced because of a continuous stream of shorter jobs.

- Average waiting time. Also once the job starts, how long does it take to complete?

- Is the waiting time bounded? This is the probability of not having to wait for some upper bound.

- Average queue length.

- The typical bound of queue length.

- Server utilization. What is the expected utilization of each server? That is, what is the amount of time it will be occupied?

If you can assign cost values to each of these considerations, we can create an analysis that will create an optimized solution to these questions. Most systems are too complex to analyze analytically, so this analysis is typically performed with simulations.

## What Do Computer Scientists Do?

Computer scientists formulate problems computationally.

### The Process for Formulating Problems Computationally

- Identify or invent useful abstractions. Create some classes that help model a problem.

- Formulate a solution to a problem as a computational experiment.

- Design and construct a succifiently efficient implementation of an experiments

- Validate the experimental setup (i.e. debug it)

- Run the experiment

- Evaluate results of the experiment

- Repeat as needed

### The Two Components of Computational Thinking

1. Abstraction. Choosing the right abstractions, and operating in terms of multiple layers of abstraction simultaneously. This involves defining the relationships between the different layers.

2. Automation. Think in terms of mechanizing these abstractions. Mechanization becomes possible because we have precise and exacting notations and models, and there is a machine that can interpret the notation.

### Examples of Computational Thinking

- How difficult is this problem, and how best can I solve it? Theoretical computer science gives precise meaning to these, and related questions and their answers.

- Thinking recursively. Reformulating a seemingly difficult problem into one which we already know how to solve. Reduction, embedding, transformation, and simulation all fit into this sort of recursive problem solving.

### Six Major Topics

1. Learning a language for expressing computations

2. Learning about the process of writing and debugging a program

3. Learning about the process of moving a problem statement to a computational formulation of a method for solving the problem

4. Learning a basic set of instructions (i.e. algorithms)

5. Learning how to use simulations to shed light on problems that don't easily succumb to closed form solutions.

6. Learning about how to use computational tools to help model and understand data.

### Writing, Testing, and Debugging Programs

- Take it one step at a time. Understand the problem, think about the overall structure and algorithms independently of expression in programming language. Break things into small, easier to understand parts. Identify useful abstractions (data and functional). Code and unit test a part at a time. Strive for functionality first, then efficiency. Start with pseudocode.

- Be systematic. When debugging apply the scientific method. Ask yourself why the program did what it did, not why it didn't do what you wanted it to.

### From Problem Statement to Computation

- Break the problem into a series of smaller problems

- Try and relate problem to a problem you or somebody else has already solved (i.e. is this a 1-0 knapsack problem?)

- Think about what kind of output you might like to seemingly

- Formulate as an optimization problem. Find the min (or max) values that satisfy some set of constraints

- Think about how to approximate solutions. Solve a simpler problem. Find a series of solutions that approaches (but may never reach) a perfect answer.

### Algorithms

- Big O Notation: Orders of growth. Exponential, polynomial, linear, log, etc. Amortized Analysis.

- Kinds of Algorithms: Exhaustive enumeration, guess and check, successive approximation, greedy algorithms, divide and conquer, decision trees, dynamic programming.

- Specific algorithms: Binary search, merge sort, etc.

- Optimization Problems: Knapsack, shortest path, dynamic programming.

### Modeling the World

- Models are always inaccurate, but often useful.

- Models provide abstractions of reality. Simulation models like Monte Carlo, and queuing networks. Statistical models like linear regression. And graph theoretic models.

### Making Sense of Data

- Statistics: uses and misuses

- Plotting

- Machine Learning. Supervised with classifications, unsupervised with hierarchical and k-means clustering, and feature vectors that abstract from data items to relevant properties. Scaling matters.

### Pervasive Themes

- Power of Abstraction

- Systematic Problem Solving
