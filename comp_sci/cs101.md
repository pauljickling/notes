# Computer Science 101

## Table of Contents

1. [Introduction](##Introduction)
2. [Problem Solving](##Problem-Solving)
3. [Functions and Machine Interpretations of Programming](##Functions-and-Machine-Interpretations-of-Programming)
4. [Objects](##Objects)
5. [Recursion](##Recursion)
6. [Debugging](##Debugging)
7. [Efficiency and Order of Growth](##Efficiency-and-Order-of-Growth)
8. [Memory and Search Methods](##Memory-and-Search-Methods)
9. [Hashing and Classes](##Hashing-and-Classes)
10. [Object Oriented Programming and Inheritance](##Object-Oriented-Programming-and-Inheritance)
11. [Intro to Simulation and Random Walks](##Intro-to-Simulation-and-Random-Walks)
12. [Basic Probability and Plotting Data](##Basic-Probability-and-Plotting-Data)

## Introduction

There are two kinds of knowledge.

1. Declarative knowledge is composed of statements of fact
2. Imperative knowledge states how to solve a problem

An algorithm is a description of how to perform a computation.

Programming languages are ways for computers to understand algorithms. They consist of instruction sets and a flow-of-control.

There are compiled and interpreted languages.

## Problem Solving

Broadly speaking, there are several types of ways to solve a problem.

### Exhausting Enumeration

Exhausting enumeration exhausts a list of possible answers within a problem set. This is colloquially referred to as *brute forcing* a solution, and this is often a good way to solve many problems since computers are fast.

### Approximation

For hard problems, like NP problems, it is more efficient to find an approximate solution that is "good enough". What does it mean for an answer to be good enough?

Lets say you are trying to solve for the square root of 2. The answer will be between 1.4 and 1.45. However, you could also be more precise and say that the answer is between 1.41 and 1.42. With approximation, you create what is called a *terminating value* where if an answer is within a predetermined range than the condition has been satisfied. This terminating condition is sometimes called the *specification* of the problem.

### Designing algorithms

When designing an algorithm you can improve efficiency by reducing the amount of input. For example, if your algorithm that is solving the square root of a number, and it considers every numerical increase (0, 0.00001, 0.00002, 0.00003, etc.) than the larger the number being tested the greater the volume of items that the algorithm needs to evaluate. There are two things you can do to improve the efficiency of such an algorithm.

1. Reduce the specification. If, for example, you instead consider (0, 0.0001, 0.0002, 0.0003, etc.) you have considerably reduced the number of items for the algorithm to consider.

2. At this point it is better to not solve the problem by brute force. A binary search method would arrive at a solution considerably faster.

## Functions and Machine Interpretations of Programming

A good programmer is measured by the functionality of their programs.

Functions solve two different issues in programming:

1. *Decomposition*: Functions create structure by breaking up the program into modules. They are self-contained and reusable.

2. *Abstraction*: Functions suppress details. They treat blocks of code as a black box where you no longer need to think about how it works, you only need to know how to use it.

Two things happen when a function is called:

1. The formal parameter is bound to the value of the actual parameter.

2. Upon entry of a function a new scope is created. A scope is a mapping from names to object.

## Objects

*Binding* is the assignment of names to objects. *Mutation* changes the value of those objects. Certain data structure collections will be immutable (Python's tuples, for example) and others are mutable (a Javascript array).

An *alias* is a single object with multiple names. For immutable objects this presents no problems, but for mutable objects this is a potential way to create annoying bugs. As an example, imagine a function that takes two arrays or lists as parameters where it appends the contents of one list and to the second. Now imagine calling that function and using the same list for both parameters. That's an example of an alias, and in this example it creates an infinite loop since while the loop that appends elements is running it will discover an ever increasing length to the list.

## Recursion

Modular abstraction is desirable because small problems are easier to solve and solutions to small problems can easily be used to solve the original problem.

Recursion is a way of describing problems and designing solutions.

First there is a base case with a direct answer. Then there is the recursive case that reduces a complex problem into a simpler one.

## Debugging

The binary number system is significant since that is the number system a classical computer (i.e. non-quantum) utilizes. Computers work in binary because of switch positions in electronics. Designing an on/off switch makes the translation simple.

The difference between the decimal and binary system causes problems found in floating numbers. For example:

Expressing 0.125 in the decimal system: (1 * 10<sup>-1</sup>) + (2 * 10<sup>-2</sup>) + (5 * 10<sup>-3</sup>)

Expressing 0.001 in binary: 1 * 10<sup>-3</sup>

Expressing 0.1 in decimal: 1 * 10<sup>-1</sup>

Expressing 0.1 in binary involves in an infinite repeating binary function.

The takeaway from this is to never evaluate if two floating numbers are equal to each other. Use a specification variable to check if they are "close enough".

When debugging a program, it is good to utilize the scientific method.

- Ask how the results could have happened?
- Study the available data
- Form a hypothesis consistent with the available data
- Design and run a repeatable experiment (the nice thing about programming is most of the time your experiment should be repeatable)
- Make sure your experiment has the potential to refute your hypothesis

Some additional considerations for writing tests:

- Try to find smaller pieces of input that fail
- It is helpful to have statements evaluating what you think should happen (i.e. `"Should print True", bool`)

## Efficiency and Order of Growth

Efficiency is about choosing the right algorithm, not about specific coding details.

Algorithms are difficult to invent. Typically when faced with a novel problem, you try to reduce the scope of the problem until it resembles an already solved problem (i.e. one where an algorithm already exists).

Efficiency problems take on two dimensions: space & time, but practically speaking usually one is concerned about time.

There is no way to make stable measurements of computational complexity. It will depend on the speed of the machine, and other highly variable factors. Instead the number of steps taken to complete an instruction set are considered.

When considering the length of time a program takes there are three cases one could look at: best case, expected case, and worst case.

Best case is not helpful. When looking for an item in a list if it happens to be the first item that doesn't tell you a lot about the efficiency of the algorithm.

The expected case is also usually ignored since there isn't any straight forward way to model it.

Instead the focus is on the worst-case scenario, which provides the upper bounds of the time it takes an algorithm to solve a problem. This is in fact a very useful scenario to consider since it comes up a lot (for example, an algorithm tries to find an item in a list that isn't actually contained in the list).

Big O notation is so widely used because it takes into account these sorts of considerations.

Big O notation as it is utilized in computer science relies on a Random Access Machine (RAM) model of how the computer performs operations. The assumption of this model is:

1. Instructions are performed sequentially
2. Any random piece of memory can be accessed in constant time.

Before the invention of the hard drive when memory for computers was stored on reel-to-reel tape this would have been a bad model to use for evaluating the efficiency of an algorithm because the location of a piece of information on the tape would be of great practical importance. And the RAM model isn't quite how computers work today, in fact computers do have different hierarchies of cached memory that take different times to access. Nonetheless, the computers we work with today bear a close resemble to the RAM model, and so for practical purposes Big O notation is the right way to evaluate the efficiency of the algorithms we write.

## Memory and Search Methods

It is difficult to evaluate the size (in memory) of an object that contains multiple data types. Some data types have a fixed size: integers are whatever bit of memory they take up (32-bit and 64-bit are the most common), booleans are also a fixed size. However we don't know the size in advance of other data types. Strings, even if they are immutable, will vary on size depending on the number of characters. And then some data types are mutable, and that means it is impossible to know their size in advance.

A linked list has every element of a list as a pointer to the next element of the list, and the value. This makes the linked list efficient to write to, but inefficient to read from. So in practice the linked list is a rarely used data structure.

The implementation arrays in most languages instead relies on what is called *indirection*, or *dereferencing*. Indirection is when a piece of data is referenced using a name instead of the value, and this is a technique that is used a lot in object oriented programming.

Binary searches are O(log n) so they are very efficient, but only if they have a sorted list to begin with. The question is is it efficient to write code where the order of operations is:

1. Sort a list
2. Run a binary search

We already know the efficiency of the binary search, so the only question is what is the rate of growth for sorting a list? To efficiently sort a list you would need to be more efficient than linear growth i.e. sublinear search. In other words, it is impossible.

### Amortized complexity

However we can say that even if the sort operation is inefficient, if one references a sorted list enough times it will nonetheless be more efficient than if we were constantly looking up the unsorted list.

If we plan on k searches of list L we are evaluating the following:

O(sort(L) + k * log(len(L) < k(len(L))))

The next question is how well can we do sorting? (the answer should always be better than bubble sort).

### Selection Sort

Selection sort depends on establishing and maintaining invariants. The list is divided into a prefix and suffix. The invariant will always have the prefix sorted.

Each step will find the lowest value, and put it as the first element. The complexity of the method is that it looks at every element of the suffix. So it looks at n elements, then n - 1 elements, then n - 2 elements, etc. It amounts to roughly O(n<sup>2</sup>).

### Merge Sort

There is a better method for sorting that relies on divide & conquer principles.

1. Choose a threshold size
2. Choose how many instances are at each division.
3. We need an algorithm to combine the sub-solutions.

Given two sorted lists you can merge them quickly.

```
l1 = [1, 5, 12, 18, 19, 20]
l2 = [2, 3, 4, 17]
```

1. Compare the first elements of each list
2. Compare the first element that "lost" to the second element of the other list.

The number of copies is O(len(list)), but the question of how many comparisons is trickier to solve, but the worst case is still O(len(list)).

How many times would you perform a merge sort? The answer is O(n log n).

## Hashing, Exceptions and Classes

### Hashing

Hashing is how objects are implemented. They are efficient, but it comes at the cost of memory/space.

`hash(i) -> 0 -> k`

The property of a hash function is many-to-one. Any number of integers can get assigned to a fixed number of buckets. When integers have the same value in the same bucket collision occurs. Linear rehashing is one of the many ways to solve this problem.

The complexity of the hash is dependent on the length of the buckets.

### Exceptions

TypeOf errors, and other kinds of errors are exceptions. Unhandled exceptions will cause the program to crash.

try-except blocks are exceptions a program is designed to accept. If the exception is raised it will jump to a different block of code to work on. This is important for web servers, and other environments where you might be expecting data that for whatever reason is not available for the program to evaluate/handle.

### Classes

A class is a collection of data and functions that operate on that data. Classes are critical for object-oriented programming.

The data and functions associated with a class' objects are called its attributes.

Sometimes when people discuss Object-Oriented Programming people will make reference to a "message passing metaphor".

A method is a function associated with an object.

## Object-Oriented Programming and Inheritance

The advantage of object oriented programming is it gives us access to abstract data types. This allows us to extend the programming language we work in by using user defined types. Each type will have an interface that explains what the methods do.

Object-Oriented languages use key words like `this` or `self` as a local type to a class that can be extended to other objects of that class.

*Data hiding* is the concept that a user shouldn't have to access instance variables contained within a class, as well as class variables.

Subclasses inherit the properties of the super class. Subclasses are useful because they produce different types of objects that can be used to distinguish objects.

For example, there could be a Person class, with a subclass of University Students. And then among University Students there could be subclasses of Graduate Students and Undergraduate Students. University Students could have ID numbers that all instances share. Undergrads could have a year attribute, but Graduates would not have this attribute.

## Intro to Simulation and Random Walks

### Generators

A generator is a function that remembers the point in the function body where it last returned, plus local variables. Generators provide a useful way to control access to a collection.

### Scientific Inquiry

How do we build computational models to solve a problem?

For much of scientific history the focus was on inventing analytic methods. That is, a way to predict outcomes given initial conditions and parameters. This lead to Newtonian physics, calculus, and probability theory. In the 20th century it reached its limits, and so simulation methods were developed. Sometimes there are non-mathematically tractable models like weather forecasting. Something with complex systems it is better to successively refine a series of simulations.

Simulations are also good at extracting useful intermediate results. The utility of simulations for scientific research were all made practical by computers. Before computers, simulations could be constructed mechanically, but this was expensive and difficult to produce, for example Leonardo da Vinci created a model of the solar system with a complicated clockwork contraption (you can see examples of devices like this in the Galileo Museum in Firenze).

A simulation builds a model that gives useful information about behavior of a system. It is an approximation to reality. Simulation models are descriptive, not prescriptive.

Brownian motion is an example of a *random walk*. A random walk assumes a randomly distributed range to try and understand behavior.

## Basic Probability and Plotting Data

How much should be inferred from simulation results? Not too much. The results of programs are *stochastic*, which is to say probabilistic.

The role of randomness in computation is challenging for human intuition. The Copenhagen doctrine put an end to the deterministic worldview.

*Casual non-determination*: Not every event is caused by previous events. Einstein hated this view and argued for predictive non-determinism, essentially the statistical theory is a result of not knowing enough information about the prior states of being.

*Stochastic Processes* are when the next state depends on the previous state(s), and there is a random element.

Independence in randomness is important, one outcome should have no influence on the next. So when asking what a probability is we're asking what fraction of possible results contain this random independence property.

Computing the odds of something happening, and the odds of something not happening are equally useful.

2<sup>10</sup> is the range of ten coin flips.

1/<sub>2<sup>10</sub> is the odds for a particular coin flip.

1 - 1/<sub>2<sup>10</sub> is the odds of a particular coin flip *not happening*.

Useful links for simulation analysis:

[SciPy](https://scipy.org/getting-started.html)

[MatPlotlib](matplotlib.sourceforge.net)
