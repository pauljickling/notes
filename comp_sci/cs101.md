# Computer Science 101

## Introduction

There are two kinds of knowledge.

1. Declarative knowledge is composed of statements of fact
2. Imperative knowledge states how to solve a problem

An algorithm is a description of how to perform a computation.

Programming languages are ways for computers to understand algorithms. They consist of instruction sets and a flow-of-control.

There are compiled and interpreted languages.

## Problem Solving

Broadly speaking, there are several types of ways to solve a problem.

### Exhausting Enumeration

Exhausting enumeration exhausts a list of possible answers within a problem set. This is colloquially referred to as *brute forcing* a solution, and this is often a good way to solve many problems since computers are fast.

### Approximation

For hard problems, like NP problems, it is more efficient to find an approximate solution that is "good enough". What does it mean for an answer to be good enough?

Lets say you are trying to solve for the square root of 2. The answer will be between 1.4 and 1.45. However, you could also be more precise and say that the answer is between 1.41 and 1.42. With approximation, you create what is called a *terminating value* where if an answer is within a predetermined range than the condition has been satisfied. This terminating condition is sometimes called the *specification* of the problem.

### Designing algorithms

When designing an algorithm you can improve efficiency by reducing the amount of input. For example, if your algorithm that is solving the square root of a number, and it considers every numerical increase (0, 0.00001, 0.00002, 0.00003, etc.) than the larger the number being tested the greater the volume of items that the algorithm needs to evaluate. There are two things you can do to improve the efficiency of such an algorithm.

1. Reduce the specification. If, for example, you instead consider (0, 0.0001, 0.0002, 0.0003, etc.) you have considerably reduced the number of items for the algorithm to consider.

2. At this point it is better to not solve the problem by brute force. A binary search method would arrive at a solution considerably faster.

## Functions and Machine Interpretations of Programming

A good programmer is measured by the functionality of their programs.

Functions solve two different issues in programming:

1. *Decomposition*: Functions create structure by breaking up the program into modules. They are self-contained and reusable.

2. *Abstraction*: Functions suppress details. They treat blocks of code as a black box where you no longer need to think about how it works, you only need to know how to use it.

Two things happen when a function is called:

1. The formal parameter is bound to the value of the actual parameter.

2. Upon entry of a function a new scope is created. A scope is a mapping from names to object.

## Objects

*Binding* is the assignment of names to objects. *Mutation* changes the value of those objects. Certain data structure collections will be immutable (Python's tuples, for example) and others are mutable (a Javascript array).

An *alias* is a single object with multiple names. For immutable objects this presents no problems, but for mutable objects this is a potential way to create annoying bugs. As an example, imagine a function that takes two arrays or lists as parameters where it appends the contents of one list and to the second. Now imagine calling that function and using the same list for both parameters. That's an example of an alias, and in this example it creates an infinite loop since while the loop that appends elements is running it will discover an ever increasing length to the list.

## Recursion

Modular abstraction is desirable because small problems are easier to solve and solutions to small problems can easily be used to solve the original problem.

Recursion is a way of describing problems and designing solutions.

First there is a base case with a direct answer. Then there is the recursive case that reduces a complex problem into a simpler one.

## Debugging

The binary number system is significant since that is the number system a classical computer (i.e. non-quantum) utilizes. Computers work in binary because of switch positions in electronics. Designing an on/off switch makes the translation simple.

The difference between the decimal and binary system causes problems found in floating numbers. For example:

Expressing 0.125 in the decimal system: (1 * 10<sup>-1</sup>) + (2 * 10<sup>-2</sup>) + (5 * 10<sup>-3</sup>)

Expressing 0.001 in binary: 1 * 10<sup>-3</sup>

Expressing 0.1 in decimal: 1 * 10<sup>-1</sup>

Expressing 0.1 in binary involves in an infinite repeating binary function.

The takeaway from this is to never evaluate if two floating numbers are equal to each other. Use a specification variable to check if they are "close enough".

When debugging a program, it is good to utilize the scientific method.

- Ask how the results could have happened?
- Study the available data
- Form a hypothesis consistent with the available data
- Design and run a repeatable experiment (the nice thing about programming is most of the time your experiment should be repeatable)
- Make sure your experiment has the potential to refute your hypothesis

Some additional considerations for writing tests:

- Try to find smaller pieces of input that fail
- It is helpful to have statements evaluating what you think should happen (i.e. `"Should print True", bool`)

## Efficiency and Order of Growth

Efficiency is about choosing the right algorithm, not about specific coding details.

Algorithms are difficult to invent. Typically when faced with a novel problem, you try to reduce the scope of the problem until it resembles an already solved problem (i.e. one where an algorithm already exists).

Efficiency problems take on two dimensions: space & time, but practically speaking usually one is concerned about time.

There is no way to make stable measurements of computational complexity. It will depend on the speed of the machine, and other highly variable factors. Instead the number of steps taken to complete an instruction set are considered.

When considering the length of time a program takes there are three cases one could look at: best case, expected case, and worst case.

Best case is not helpful. When looking for an item in a list if it happens to be the first item that doesn't tell you a lot about the efficiency of the algorithm.

The expected case is also usually ignored since there isn't any straight forward way to model it.

Instead the focus is on the worst-case scenario, which provides the upper bounds of the time it takes an algorithm to solve a problem. This is in fact a very useful scenario to consider since it comes up a lot (for example, an algorithm tries to find an item in a list that isn't actually contained in the list).

Big O notation is so widely used because it takes into account these sorts of considerations.

Big O notation as it is utilized in computer science relies on a Random Access Machine (RAM) model of how the computer performs operations. The assumption of this model is:

1. Instructions are performed sequentially
2. Any random piece of memory can be accessed in constant time.

Before the invention of the hard drive when memory for computers was stored on reel-to-reel tape this would have been a bad model to use for evaluating the efficiency of an algorithm because the location of a piece of information on the tape would be of great practical importance. And the RAM model isn't quite how computers work today, in fact computers do have different hierarchies of cached memory that take different times to access. Nonetheless, the computers we work with today bear a close resemble to the RAM model, and so for practical purposes Big O notation is the right way to evaluate the efficiency of the algorithms we write.

## Memory and Search Methods

It is difficult to evaluate the size (in memory) of an object that contains multiple data types. Some data types have a fixed size: integers are whatever bit of memory they take up (32-bit and 64-bit are the most common), booleans are also a fixed size. However we don't know the size in advance of other data types. Strings, even if they are immutable, will vary on size depending on the number of characters. And then some data types are mutable, and that means it is impossible to know their size in advance.

A linked list has every element of a list as a pointer to the next element of the list, and the value. This makes the linked list efficient to write to, but inefficient to read from. So in practice the linked list is a rarely used data structure.

The implementation arrays in most languages instead relies on what is called *indirection*, or *dereferencing*. Indirection is when a piece of data is referenced using a name instead of the value, and this is a technique that is used a lot in object oriented programming.

Binary searches are O(log n) so they are very efficient, but only if they have a sorted list to begin with. The question is is it efficient to write code where the order of operations is:

1. Sort a list
2. Run a binary search

We already know the efficiency of the binary search, so the only question is what is the rate of growth for sorting a list? To efficiently sort a list you would need to be more efficient than linear growth i.e. sublinear search. In other words, it is impossible.

### Amortized complexity

However we can say that even if the sort operation is inefficient, if one references a sorted list enough times it will nonetheless be more efficient than if we were constantly looking up the unsorted list.

If we plan on k searches of list L we are evaluating the following:

O(sort(L) + k * log(len(L) < k(len(L))))

The next question is how well can we do sorting? (the answer should always be better than bubble sort).

### Selection Sort

Selection sort depends on establishing and maintaining invariants. The list is divided into a prefix and suffix. The invariant will always have the prefix sorted.

Each step will find the lowest value, and put it as the first element. The complexity of the method is that it looks at every element of the suffix. So it looks at n elements, then n - 1 elements, then n - 2 elements, etc. It amounts to roughly O(n<sup>2</sup>).

### Merge Sort

There is a better method for sorting that relies on divide & conquer principles.

1. Choose a threshold size
2. Choose how many instances are at each division.
3. We need an algorithm to combine the sub-solutions.

Given two sorted lists you can merge them quickly.

```
l1 = [1, 5, 12, 18, 19, 20]
l2 = [2, 3, 4, 17]
```

1. Compare the first elements of each list
2. Compare the first element that "lost" to the second element of the other list.

The number of copies is O(len(list)), but the question of how many comparisons is trickier to solve, but the worst case is still O(len(list)).

How many times would you perform a merge sort? The answer is O(n log n).

## Hashing, Exceptions and Classes

### Hashing

Hashing is how objects are implemented. They are efficient, but it comes at the cost of memory/space.

`hash(i) -> 0 -> k`

The property of a hash function is many-to-one. Any number of integers can get assigned to a fixed number of buckets. When integers have the same value in the same bucket collision occurs. Linear rehashing is one of the many ways to solve this problem.

The complexity of the hash is dependent on the length of the buckets.

### Exceptions

TypeOf errors, and other kinds of errors are exceptions. Unhandled exceptions will cause the program to crash.

try-except blocks are exceptions a program is designed to accept. If the exception is raised it will jump to a different block of code to work on. This is important for web servers, and other environments where you might be expecting data that for whatever reason is not available for the program to evaluate/handle.

### Classes

A class is a collection of data and functions that operate on that data. Classes are critical for object-oriented programming.

The data and functions associated with a class' objects are called its attributes.

Sometimes when people discuss Object-Oriented Programming people will make reference to a "message passing metaphor".

A method is a function associated with an object.

## Object-Oriented Programming and Inheritance

The advantage of object oriented programming is it gives us access to abstract data types. This allows us to extend the programming language we work in by using user defined types. Each type will have an interface that explains what the methods do.

Object-Oriented languages use key words like `this` or `self` as a local type to a class that can be extended to other objects of that class.

*Data hiding* is the concept that a user shouldn't have to access instance variables contained within a class, as well as class variables.

Subclasses inherit the properties of the super class. Subclasses are useful because they produce different types of objects that can be used to distinguish objects.

For example, there could be a Person class, with a subclass of University Students. And then among University Students there could be subclasses of Graduate Students and Undergraduate Students. University Students could have ID numbers that all instances share. Undergrads could have a year attribute, but Graduates would not have this attribute.

## Intro to Simulation and Random Walks

### Generators

A generator is a function that remembers the point in the function body where it last returned, plus local variables. Generators provide a useful way to control access to a collection.

### Scientific Inquiry

How do we build computational models to solve a problem?

For much of scientific history the focus was on inventing analytic methods. That is, a way to predict outcomes given initial conditions and parameters. This lead to Newtonian physics, calculus, and probability theory. In the 20th century it reached its limits, and so simulation methods were developed. Sometimes there are non-mathematically tractable models like weather forecasting. Something with complex systems it is better to successively refine a series of simulations.

Simulations are also good at extracting useful intermediate results. The utility of simulations for scientific research were all made practical by computers. Before computers, simulations could be constructed mechanically, but this was expensive and difficult to produce, for example Leonardo da Vinci created a model of the solar system with a complicated clockwork contraption (you can see examples of devices like this in the Galileo Museum in Firenze).

A simulation builds a model that gives useful information about behavior of a system. It is an approximation to reality. Simulation models are descriptive, not prescriptive.

Brownian motion is an example of a *random walk*. A random walk assumes a randomly distributed range to try and understand behavior.

## Basic Probability and Plotting Data

How much should be inferred from simulation results? Not too much. The results of programs are *stochastic*, which is to say probabilistic.

The role of randomness in computation is challenging for human intuition. The Copenhagen doctrine put an end to the deterministic worldview.

*Casual non-determination*: Not every event is caused by previous events. Einstein hated this view and argued for predictive non-determinism, essentially the statistical theory is a result of not knowing enough information about the prior states of being.

*Stochastic Processes* are when the next state depends on the previous state(s), and there is a random element.

Independence in randomness is important, one outcome should have no influence on the next. So when asking what a probability is we're asking what fraction of possible results contain this random independence property.

Computing the odds of something happening, and the odds of something not happening are equally useful.

2<sup>10</sup> is the range of ten coin flips.

1/<sub>2<sup>10</sub> is the odds for a particular coin flip.

1 - 1/<sub>2<sup>10</sub> is the odds of a particular coin flip *not happening*.

Useful links for simulation analysis:

[SciPy](https://scipy.org/getting-started.html)

[MatPlotlib](matplotlib.sourceforge.net)

## Sampling and Monte Carlo Simulation

If a die is rolled 10 times what is the probability that a 1 will never be rolled? There are 6<sup>10</sup> possible permutations. So then the question is among those collections, how many instances don't contain a 1?

In a single roll the odds are 5/<sub>6</sub>. On the second roll the odds are 5/<sub>6</sub> * 5/<sub>6</sub>. On the third roll the odds are 5/<sub>6</sub> * 5/<sub>6</sub> * 5/<sub>6</sub>. etc.

Therefore the odds of getting at least one 1 is 1 - (5/<sub>6</sub>)<sup>10</sup>.

Solving a probability question by solving for not x or - 1 is a common technique.

Almost all of early probability theory was dedicated to solving gambling problems. Pascal is considered foundational to the field. A typical inquiry would be is it profitable to roll a pair of dice 24 times to roll a pair of sixes?

1/<sub>6</sub> * 1/<sub>6</sub> = 1/<sub>36</sub> for the first roll. So (35/<sub>36</sub>)<sup>24</sup> is roughly equal to 0.51.

Is it easier to write a simulation, or to calculate the probabilities? If possible, doing both is the best approach since it will help verify your work.

A *Monte Carlo simulation* uses repeated sampling to obtain the statistical properties of some phenomenon. The phrase was coined in 1949 to refer to a method from 1946.

*Inferential statistics* is based upon the guiding principle that a random sample tends to exhibit the same properties as the population from which it is drawn.

It is important to raise questions about sampling methods. Incorrect sampling will lead you astray.

The greater the number of simulations, the more stable the range of results. This is also known as *the law of large numbers*. It states that in repeated *independent* tests with the same actual probability (p) of an outcome for each test, the chance that the fraction of times that outcome occurs converges to p as the number of trials goes to infinite.

You can never be totally certain about sampling results. What techniques can be used to determine certainty within a range?

## Statistical Thinking

How many samples do we need to believe an answer?

Variance is the measure of how much spread there is in the possible outcomes. Thus multiple trials are critical. One trial that makes one hundred attempts is not as good as ten trials that make ten attempts.

Standard deviation measures the fraction of values that are close to the mean.

The coefficient of variation is used to measure whether the variation is relatively acceptable. It is the standard deviation divided by the mean.

If standard deviation is less than 1 it is low variance. If the mean is 0 small changes will lead to large variance, but that isn't really meaningful.

The coefficient of variation cannot be used for confidence intervals.

Normal distribution always peaks at the mean, and falls off symmetrically. The shape is a bell curve.

Normal distributions are used to construct models because:

1. they have nice mathematical properties
2. there are many naturally occuring instances.

They are characterized by two parameters: the mean, and the standard deviation. Knowing these we can calculate confidence intervals.

A confidence interval creates estimates by providing a range likely to contain the unknown value, and a confidence level that the value lies within that range.

*Example*: Candidate (D) likely to get 52% ± 4% of votes.

Note that this assumes that trials of elections have normal distributions.

*The empricial rule*: 68% of data is within 1 standard deviation of means.

The standard error is an estimate of the standard deviation. It assumes a normal distribution and that the sample population is small relative to the actual population.

If we assume:

p= % of population sampled
n= sample size

then...
standard error = ((p * (100 - p) / n))

Many experimental setups have normally distributed measurement errors.

## Using Randomness to Solve Non-Random Problems

How do we construct computational models that we can use to understand the real world?

A Gaussian distribution is one way to understand possible outcomes. Another name for this is a normal distribution, and it is also informally referred to as the bell curve.

A uniform distribution is where each result is equally probable. The range is all that you need to know about a uniform distribution. This type of distribution is common in gambling systems, but not in naturally occuring systems which tend to have a normal distribution.

Exponential distributions have the property of being memory-less. At each step the property is independent of previous steps.

Step 1: (1 - p)
Step 2: (1 - p)<sup>2</sup>
Time T: (1 - p)<sup>T</sup>

In evaluating models we evaluate fidelity (i.e. the credibility of the model, and utility i.e. what questions are answerable with the model?)

In contrast to analytic models, simulation models often have additional utility, but they often have comparable fidelity.

Many physical systems involve exponential decay and growth, for instance half-life decay or algae population growth.

Randomized algorithms have a purpose for non-random problems.

Pi is a known number that mathematicians have tried to solve for. Egyptians figured it to be 3.16, which was a fairly good answer. The Bible supposed it to be 3 (less good). Archimedes tried something slightly more sophisticated and created an approximation providing an upper and lower bound.

223/<sub>71</sub> < Pi < 22/<sub7</sub> ≈ 3.1418

In the 17th century two French mathematicians created a stochastic simulation. They created a circle within a square, and dropped needles in, counting the number of needles inside the circle, and the number outside the circle but within the square. From that they derived the formula:

needles in circle/<sub>needles in square</sub> = area of circle/<sub>area of square</sub>

From that formula they reasoned:

Pi = area of circle = area of square * needles in circle/<sub>needles in square</sub>

They ran into a problem though. They didn't have enough needles to arrive at a good estimate. However the model for the simulation was sound. The more simulations you can run, the smaller the standard deviation becomes, which increases your confidence in the answer.

## Curve Fitting

Modern science considers the interplay of physical reality, theoretical models, and computational models.

Hook's law calculates spring force, which turns out to have a lot of applications. The formula is:

f - kx

Where f is the force, and k is the spring constant. If you take a spring and attach a weight of a known quantity you can calculate for k.

Experimental errors are a practical reality. So using multiple weights to give you more data points is valuable.

To determine the best fit for data we need an objective function. There is a standard typically used called the *least squares fit*. It amounts to the formula:

(observed points[i] - predicted points[i])<sup>2</sup>

For every x value there should be a predicted y value.

polyFit is a built in function to pyLabs that finds a fit. It takes 3 arguments:

1. All of the observed x values
2. All of the observed y values
3. The degree of the polynomial

Possible outcomes could include:

- A line (y = ax + b)
- A parabola (y = ax<sup>2</sup> + bx<sup>2</sup> + c)

Linear regression can be used to find polynomials besides lines.

In addition to linear fit there is also cubic fits, etc.

It is possible to fit curves that have terrible predictive value. This is an issue that cannot be answered by statistics. One has to return to what the theoretical proposition states. There is a formula called the coefficient determination that is used to determine the quality of a fit. It is represented by R<sup>2</sup>, and the formula is:

R<sup>2</sup> = 1 - EE/<sub>MV</sub>

Where EE stands for estimated error, and MV stands for measured variance. This produces a value between 1 and 0, where 1 represents absolute correlation, and 0 represents no correlation at all.

Once you have a good model you can ask why do we care? The purpose is to use the model to ask questions about real instances. The pattern that is used in scientific and engineering experiments is as follows:

1. Start with an experiment that provides you with data about the behavior of a particular system.

2. Use computation to find and evaluate a model. Evaluation is important because we care about the quality and validity of the model.

3. Use theory, analysis, and additional computation to derive a consequence of the model.

## Optimization Problems and Algorithms

Optimization problems, as the name suggests, are problems where you are trying to perform some actions that need to be optimized. They consist of two parts:

1. An *objective function*, that is, what is the thing you are trying to optimize? You are going to look at either maximizing or minimizing some value. For example, what is the fastest way to arrive from point A to point B using public transportation? Alternatively you could be looking to minimize the amount of money you are spending.

2. A *set of constraints*. For example, you cannot spend more than a fixed amount of money travelling a particular distance.

Like other problems, the typical approach to take is one of *problem reduction*, that is, take a new problem and map it to an existing, solved problem.

With optimization problems you need to think about how hard they are. Optimization problems can feature NP complexity, and in those instances you may need to find approximate answers so that the problem can be solved in a reasonable amount of time.

A classic optimization problem is *the knapsack problem*. In the knapsack problem you can fix a certain amount of items of various values and weights into a knapsack that has a particular weight capacity. How do you maximize the value of the items that are going into the knapsack?

The first approach is to use a greedy algorithm. A greedy algorithm has a characteristic of being iterative, and simply searching for a locally optimal solution, that is, it only cares about the optimal solution at any particular step. This immediately begs the question of what counts as locally optimal?

There are in fact different types of knapsack problems. There is the 0/1 knapsack problem, as well as the continuous knapsack problem. The 0/1 problem involves taking whole items, so an item is either taken, or it isn't. The continuous knapsack problem doesn't have this constraint, and therefore is easier to solve (you take as much of the most valuable item as you can, then move onto the next most valuable, etc.) We will be looking at the 0/1 problem since that poses more interesting questions.

A greedy algorithm function could have 3 parameters: items, knapsack carry capacity, and a "key function". The key function is simply a definition of what counts as locally optimal. You could have three possible theories of what counts as locally optimal:

1. Select the item of the greatest value.

2. Select the item with the least weight so that you can maximize the items you pick up.

3. Calculate and select items that provide the best value/weight ratio.

The efficiency of a greedy algorithm is close to O(n), so it is very efficient. This is the nice thing about greedy algorithms, they are efficient and easy to code. The downside is they do not always result in what is globally optimal. One issue the knapsack problem exposes is that what counts as locally optimal will vary depending on the carry capacity of the knapsack. So in this instance it is worth thinking more about how we might formulate what is globally optimal.

One way is to brute force the problem. We can take the following steps:

1. Represent each item by a pair where I = (value, weight)

2. Have W as the carry capacity for the knapsack.

3. Have an available items vector called AI.

4. Have a selected items vector called SI.

So the new objective function will have the following formula:

`SI[i] * AI[i].value`

And it will be subject to the following constraint:

`AI[i] * SI[i].weight <= W`

What is the efficiency of this? Stated another way, how many possible vectors are there? It will be O(2<sup>n</sup>). If n is 5 this might not be so bad, but if it is 500 this is incredibly bad, and cannot be solved in a reasonable amount of time.

To solve this problem we will end up relying on the concept of a *powerset* which is the set of all subsets. The knapsack problem is inherently exponential. That is to say, the *worst case scenario* is inherently exponential. Clever algorithms can get around this by avoiding the worst case scenario.
